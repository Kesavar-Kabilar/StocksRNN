{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stocks RNN Predictor\n",
        "\n",
        "By: Kesavar Kabilar, Ryan James Laporte, and Carmelo Restivo-Caponcello"
      ],
      "metadata": {
        "id": "mtQSvAVHau3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "zcBloFZZlij0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vHO7xjuFlYJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7842ee44-927a-424b-c504-70088cda73c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7)"
      ],
      "metadata": {
        "id": "FkZCrCdRQ4oO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/gdrive/My Drive/Project/Stocks/\" # Change file directory"
      ],
      "metadata": {
        "id": "MX5ZADY6sSHI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_files(data_path=data_path):\n",
        "    \"\"\"\n",
        "    Given a path to the stocks data, this function will return the file \n",
        "    locations of companies consisting of at least 100 lines of previous stocks \n",
        "    data.\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "\n",
        "    for file in os.listdir(data_path)[:100]:\n",
        "        f = open(data_path+file)\n",
        "        if len(f.readlines()) > 100: \n",
        "            all_files.append(data_path+file)\n",
        "        f.close()\n",
        "    \n",
        "    return all_files\n",
        "\n",
        "all_files = get_all_files()"
      ],
      "metadata": {
        "id": "BL3QKdpcxafy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = len(all_files)\n",
        "\n",
        "all_files = np.array(all_files, dtype=str)\n",
        "np.random.shuffle(all_files)\n",
        "\n",
        "training_files = all_files[:int(n*0.6)] \n",
        "validation_files = all_files[int(n*0.6):int(n*0.8)]\n",
        "test_files = all_files[int(n*0.8):]"
      ],
      "metadata": {
        "id": "DjWTGCaO-fDn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(file, K, augment=False):\n",
        "    \"\"\"\n",
        "    Given a file, a K value it will return X, T. Each row of X contains K x 4 \n",
        "    values where each row represents the open value, highest price, lowest \n",
        "    price, and the closing price. Each column represent the previous K days. \n",
        "    Each row of T represents the closing price of K+1 day. If augment is true, \n",
        "    the function will also add the reverse of the days.\n",
        "    \"\"\"\n",
        "    X, T = [], []\n",
        "\n",
        "    f = open(file, \"r\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    data = []\n",
        "    for line in f.readlines()[-100:]:\n",
        "        line = line.split(\",\")[1:-2]\n",
        "        all_data.append(line)\n",
        "\n",
        "        data.append(line)\n",
        "        if len(data) == K+1:\n",
        "            x = np.array(data[:-1], dtype=float)\n",
        "            X.append(x)\n",
        "            T.append(float(data[-1][-1]))\n",
        "\n",
        "            data = data[1:]\n",
        "\n",
        "    f.close()\n",
        "    if augment:\n",
        "        data = []\n",
        "        for line in all_data[::-1]:\n",
        "\n",
        "            data.append(line)\n",
        "            if len(data) == K+1:\n",
        "                x = np.array(data[:-1], dtype=float)\n",
        "                X.append(x)\n",
        "                T.append(float(data[-1][-1]))\n",
        "\n",
        "                data = data[1:]\n",
        "    \n",
        "    return torch.tensor(np.array(X), dtype=torch.float), torch.tensor(np.array(T), dtype=torch.float)"
      ],
      "metadata": {
        "id": "y7eK1ZeWxzs8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StocksRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(StocksRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # normalize input\n",
        "        s1, s2, s3 = x.shape\n",
        "        x = x.reshape(s1, s2*s3)\n",
        "        min, max = torch.amin(x, axis=1), torch.amax(x, axis=1)\n",
        "        x = (x-min[:, np.newaxis]) / (max[:, np.newaxis] - min[:, np.newaxis])\n",
        "        x = x.reshape(s1, s2, s3)\n",
        "        # RNN Layer\n",
        "        out, _ = self.rnn(x)\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        # Denormalize output\n",
        "        return ((out.reshape(-1) * (max-min)) + min).reshape(-1, 1)\n",
        "\n",
        "class StocksLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(StocksLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # normalize input\n",
        "        s1, s2, s3 = x.shape\n",
        "        x = x.reshape(s1, s2*s3)\n",
        "        min, max = torch.amin(x, axis=1), torch.amax(x, axis=1)\n",
        "        x = (x-min[:, np.newaxis]) / (max[:, np.newaxis] - min[:, np.newaxis])\n",
        "        x = x.reshape(s1, s2, s3)\n",
        "        # LSTM Layer\n",
        "        out, _ = self.lstm(x)\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        # Denormalize output\n",
        "        return ((out.reshape(-1) * (max-min)) + min).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "t1Mjc2hMOe1L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mean_squared_error(model, files, K, incrDecr=False):\n",
        "    \"\"\"\n",
        "    Given the model, a set of files, and K value, The function will calculate \n",
        "    the mean squared error of the dataset.\n",
        "    \"\"\"\n",
        "    error = 0\n",
        "    total = 0\n",
        "\n",
        "    for f in files:\n",
        "        x, t = transform_data(f, K)\n",
        "        \n",
        "        if incrDecr:\n",
        "            output = model(x).reshape(-1)\n",
        "\n",
        "            t[np.where(x[:, -1, -1] >= t)] = 0\n",
        "            t[np.where(x[:, -1, -1] < t)] = 1\n",
        "            output[np.where(x[:, -1, -1] >= output)] = 0\n",
        "            output[np.where(x[:, -1, -1] < output)] = 1\n",
        "\n",
        "        else:\n",
        "            s1, s2, s3 = x.shape\n",
        "            x = x.reshape(s1, s2*s3)\n",
        "            min, max = torch.amin(x, axis=1), torch.amax(x, axis=1)\n",
        "            x = (x-min[:, np.newaxis]) / (max[:, np.newaxis] - min[:, np.newaxis])\n",
        "            x = x.reshape(s1, s2, s3)\n",
        "            t = (t-min) / (max - min)\n",
        "\n",
        "            output = model(x).reshape(-1)\n",
        "\n",
        "        error += int(sum((output - t)**2).detach())\n",
        "        total += output.shape[0]\n",
        "\n",
        "    return error / total"
      ],
      "metadata": {
        "id": "q0KTGxKf9_Ti"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_rnn_network(model, train, valid, num_epochs, batch_size, learning_rate, K, name, augment=False, show_data=True):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "\n",
        "    iter_x = []\n",
        "    iter_t = []\n",
        "    for f in train:\n",
        "        x, t = transform_data(f, K, augment)\n",
        "        iter_x.append(x)\n",
        "        iter_t.append(t)\n",
        "\n",
        "    train_data_x = torch.concat(iter_x, 0)\n",
        "    train_data_t = torch.concat(iter_t, 0)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        p = np.random.permutation(train_data_t.shape[0])\n",
        "        X, T = train_data_x[p], train_data_t[p]\n",
        "\n",
        "        for i in range(0, int(T.shape[0]), batch_size):\n",
        "            batch_x = X[i: (i+batch_size)]\n",
        "            batch_t = T[i: (i+batch_size)]\n",
        "            pred = model(batch_x)\n",
        "            loss = criterion(pred, batch_t.reshape(-1, 1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        losses.append(float(loss))\n",
        "\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_mean_squared_error(model, train, K))\n",
        "        valid_acc.append(get_mean_squared_error(model, valid, K))\n",
        "        if (epoch+1) % 10 == 0 and show_data:\n",
        "            print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "                epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "\n",
        "    if show_data:\n",
        "        plt.title(name + \" Training Curve\")\n",
        "        plt.plot(epochs, train_acc, label=\"Train\")\n",
        "        plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Mean Squared Error\")\n",
        "        plt.legend(loc='best')\n",
        "        plt.savefig(name.replace(\" \", \"\")+\".png\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ToPMEfzwS7kU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelRNN = StocksRNN(4, 150)\n",
        "train_rnn_network(modelRNN, \n",
        "                  training_files, \n",
        "                  validation_files, \n",
        "                  num_epochs=100, \n",
        "                  batch_size=500, \n",
        "                  learning_rate=1e-5,\n",
        "                  K=5, \n",
        "                  name=\"RNN Model\")\n",
        "print(\"Test Data Error: \", get_mean_squared_error(modelRNN, test_files, 5))\n",
        "print(\"Test Data Increase/Decrease Accuracy: \", get_mean_squared_error(modelRNN, test_files, 5, incrDecr=True))"
      ],
      "metadata": {
        "id": "_7GUtTNSmfwW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelLSTM = StocksLSTM(4, 150)\n",
        "train_rnn_network(modelLSTM, \n",
        "                  training_files, \n",
        "                  validation_files, \n",
        "                  num_epochs=100, \n",
        "                  batch_size=500, \n",
        "                  learning_rate=1e-5,\n",
        "                  K=5, \n",
        "                  name=\"LSTM Model\")\n",
        "print(\"Test Data Accuracy: \", get_mean_squared_error(modelLSTM, test_files, 5))\n",
        "print(\"Test Data Increase/Decrease Accuracy: \", get_mean_squared_error(modelLSTM, test_files, 5, incrDecr=True))"
      ],
      "metadata": {
        "id": "rQmB8mJ6lyHX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelRNNAug = StocksRNN(4, 150)\n",
        "train_rnn_network(modelRNNAug, \n",
        "                  training_files, \n",
        "                  validation_files, \n",
        "                  num_epochs=100, \n",
        "                  batch_size=500, \n",
        "                  learning_rate=1e-5,\n",
        "                  K=5, \n",
        "                  name=\"RNN Model Augmented\", \n",
        "                  augment=True)\n",
        "print(\"Test Data Accuracy: \", get_mean_squared_error(modelRNNAug, test_files, 5))\n",
        "print(\"Test Data Increase/Decrease Accuracy: \", get_mean_squared_error(modelRNNAug, test_files, 5, incrDecr=True))"
      ],
      "metadata": {
        "id": "juXavcazu-8M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelLSTMAug = StocksLSTM(4, 150)\n",
        "train_rnn_network(modelLSTMAug, \n",
        "                  training_files, \n",
        "                  validation_files, \n",
        "                  num_epochs=100, \n",
        "                  batch_size=500, \n",
        "                  learning_rate=1e-5,\n",
        "                  K=5, \n",
        "                  name=\"LSTM Model Augmented\", \n",
        "                  augment=True)\n",
        "print(\"Test Data Accuracy: \", get_mean_squared_error(modelLSTMAug, test_files, 5))\n",
        "print(\"Test Data Increase/Decrease Accuracy: \", get_mean_squared_error(modelLSTMAug, test_files, 5, incrDecr=True))"
      ],
      "metadata": {
        "id": "nUVxDbQ7vQPl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def goodBadExample(model, K, test_files=test_files):\n",
        "    \"\"\"\n",
        "    Given a model, K, and test_files dataset, this function will output the \n",
        "    company with the worst mean squared error and the company with the best \n",
        "    mean squared error. Display a graph for which it least correctly predicted \n",
        "    the values and a graph for which it most correctly predicted the values.\n",
        "    \"\"\"\n",
        "    bad_error, bad_file = float(\"-inf\"), \"\"\n",
        "    good_error, good_file = float(\"inf\"), \"\"\n",
        "\n",
        "    for f in test_files:\n",
        "        error = get_mean_squared_error(model, [f], K)\n",
        "        if error > bad_error:\n",
        "            bad_error = error\n",
        "            bad_file = f\n",
        "        \n",
        "        if error < good_error:\n",
        "            good_error = error\n",
        "            good_file = f\n",
        "\n",
        "    X, T = transform_data(good_file, K)\n",
        "    Y = model(X)\n",
        "\n",
        "    plt.title(\"Best Model Prediction\")\n",
        "    plt.ylabel(\"Stock Price ($)\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.plot(np.arange(len(Y)), Y.detach().numpy(), label=\"Model Prediction\")\n",
        "    plt.plot(np.arange(len(T)), T, label=\"Target Value\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"BestModelPrediction.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Best Model Prediction Error: \", \n",
        "          get_mean_squared_error(model, [good_file], K))\n",
        "\n",
        "    X, T = transform_data(bad_file, K)\n",
        "    Y = model(X)\n",
        "\n",
        "    plt.title(\"Worst Model Prediction\")\n",
        "    plt.ylabel(\"Stock Price ($)\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.plot(np.arange(len(Y)), Y.detach().numpy(), label=\"Model Prediction\")\n",
        "    plt.plot(np.arange(len(T)), T, label=\"Target Value\")\n",
        "    plt.legend()\n",
        "    plt.savefig(\"WorstModelPrediction.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Worst Model Prediction Error: \", \n",
        "          get_mean_squared_error(model, [bad_file], K))\n",
        "\n",
        "# goodBadExample(modelRNN, 5, test_files=test_files)"
      ],
      "metadata": {
        "id": "GRgbi11p4wU_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testModels(modelType):\n",
        "    print(modelType, \"Model\")\n",
        "    print(\"Epochs\\tBatch Size\\tNum Days\\tTraining Error\\tValidation Error\\tTest Error\\tClassification Error\")\n",
        "    for epochs in [50, 100, 200]:\n",
        "        for batch_size in [500, 1000]:\n",
        "            for K in [5, 10, 15]:\n",
        "                print(str(epochs) + \"\\t\" + str(batch_size) + \"\\t\\t\" + str(K) + \"\\t\\t\", end=\"\")\n",
        "                if modelType == \"RNN\":\n",
        "                    model = StocksRNN(4, 150)\n",
        "                else:\n",
        "                    model = StocksLSTM(4, 150)\n",
        "                train_rnn_network(model, \n",
        "                                  training_files, \n",
        "                                  validation_files, \n",
        "                                  num_epochs=epochs, \n",
        "                                  batch_size=batch_size, \n",
        "                                  learning_rate=1e-5,\n",
        "                                  K=K, \n",
        "                                  name=\"RNN Model\", \n",
        "                                  show_data=False)\n",
        "                train_acc = get_mean_squared_error(model, training_files, K)\n",
        "                valid_acc = get_mean_squared_error(model, validation_files, K)\n",
        "                test_acc = get_mean_squared_error(model, test_files, K)\n",
        "                testc_acc = get_mean_squared_error(model, test_files, K, incrDecr=True)\n",
        "                print(f'{train_acc:.6f}' + \"\\t\", end=\"\")\n",
        "                print(f'{valid_acc:.6f}' + \"\\t\\t\", end=\"\")\n",
        "                print(f'{test_acc:.6f}' + \"\\t\", end=\"\")\n",
        "                print(f'{testc_acc:.6f}' + \"\\t\")"
      ],
      "metadata": {
        "id": "N2sAtVPHCFtz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testModels(\"LSTM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nn81b9kOU2U5",
        "outputId": "0313c17d-4e5a-4f15-ddfc-90ea26453746"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Model\n",
            "Epochs\tBatch Size\tNum Days\tTraining Error\tValidation Error\tTest Error\tClassification Error\n",
            "50\t500\t\t5\t\t0.312865\t0.316374\t\t0.283657\t0.470360\t\n",
            "50\t500\t\t10\t\t0.242181\t0.254938\t\t0.222222\t0.482456\t\n",
            "50\t500\t\t15\t\t0.152723\t0.164052\t\t0.147988\t0.490402\t\n",
            "50\t1000\t\t5\t\t0.402729\t0.417544\t\t0.374515\t0.477008\t\n",
            "50\t1000\t\t10\t\t0.333951\t0.361728\t\t0.316374\t0.482456\t\n",
            "50\t1000\t\t15\t\t0.311329\t0.342484\t\t0.313313\t0.489164\t\n",
            "100\t500\t\t5\t\t0.252242\t0.239181\t\t0.221053\t0.475346\t\n",
            "100\t500\t\t10\t\t0.150000\t0.139506\t\t0.129240\t0.486550\t\n",
            "100\t500\t\t15\t\t0.114161\t0.111111\t\t0.104644\t0.500310\t\n",
            "100\t1000\t\t5\t\t0.345809\t0.354971\t\t0.316343\t0.458726\t\n",
            "100\t1000\t\t10\t\t0.260288\t0.275926\t\t0.243860\t0.485380\t\n",
            "100\t1000\t\t15\t\t0.195643\t0.216340\t\t0.195046\t0.476161\t\n",
            "200\t500\t\t5\t\t0.244444\t0.227485\t\t0.213296\t0.485319\t\n",
            "200\t500\t\t10\t\t0.136008\t0.120370\t\t0.114035\t0.480702\t\n",
            "200\t500\t\t15\t\t0.097821\t0.094118\t\t0.087926\t0.490402\t\n",
            "200\t1000\t\t5\t\t0.247953\t0.232164\t\t0.216066\t0.489751\t\n",
            "200\t1000\t\t10\t\t0.154321\t0.141975\t\t0.132164\t0.485965\t\n",
            "200\t1000\t\t15\t\t0.114597\t0.109804\t\t0.104025\t0.500310\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testModels(\"RNN\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yEdUPJxFwjd",
        "outputId": "39feddd4-63d4-4f79-bc36-898e2205c940"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN Model\n",
            "Epochs\tBatch Size\tNum Days\tTraining Error\tValidation Error\tTest Error\tClassification Error\n",
            "50\t500\t\t5\t\t0.239376\t0.225731\t\t0.208310\t0.489197\t\n",
            "50\t500\t\t10\t\t0.142181\t0.130247\t\t0.119298\t0.486550\t\n",
            "50\t500\t\t15\t\t0.130501\t0.126797\t\t0.119505\t0.501548\t\n",
            "50\t1000\t\t5\t\t0.264717\t0.258480\t\t0.232687\t0.472022\t\n",
            "50\t1000\t\t10\t\t0.255144\t0.269753\t\t0.238596\t0.483626\t\n",
            "50\t1000\t\t15\t\t0.102614\t0.103922\t\t0.095356\t0.488545\t\n",
            "100\t500\t\t5\t\t0.223197\t0.209357\t\t0.193352\t0.478116\t\n",
            "100\t500\t\t10\t\t0.140535\t0.130247\t\t0.119883\t0.481871\t\n",
            "100\t500\t\t15\t\t0.089760\t0.085621\t\t0.081734\t0.484830\t\n",
            "100\t1000\t\t5\t\t0.230214\t0.212865\t\t0.199446\t0.483102\t\n",
            "100\t1000\t\t10\t\t0.155350\t0.144444\t\t0.133333\t0.484211\t\n",
            "100\t1000\t\t15\t\t0.099564\t0.094118\t\t0.090402\t0.492879\t\n",
            "200\t500\t\t5\t\t0.210721\t0.195322\t\t0.182271\t0.469806\t\n",
            "200\t500\t\t10\t\t0.113169\t0.103704\t\t0.092398\t0.485380\t\n",
            "200\t500\t\t15\t\t0.077342\t0.076471\t\t0.070588\t0.481734\t\n",
            "200\t1000\t\t5\t\t0.232164\t0.215789\t\t0.201108\t0.479778\t\n",
            "200\t1000\t\t10\t\t0.130658\t0.119753\t\t0.110526\t0.485380\t\n",
            "200\t1000\t\t15\t\t0.087364\t0.084967\t\t0.081734\t0.483591\t\n"
          ]
        }
      ]
    }
  ]
}